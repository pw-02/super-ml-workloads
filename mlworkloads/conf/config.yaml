
defaults:
  - _self_
  - dataloader: super #pytorch, super, dali
  - workload: cifar10_resnet18  #imagenet_resnet50, cifar10_resnet18, openwebtext
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled  

seed: 42 # Seed for reproducibility
accelerator: 'gpu' # 'cpu', 'gpu', 'tpu'
devices: 1 # Number of devices to use for training (e.g. GPUs) per job
log_interval: 1 # Log every N steps
log_dir: logs # Directory to save logs
checkpoint_dir: checkpoints # Save a checkpoint every N steps
precision: 32 # 16, 32, 64

# num_workers_per_job: 0 # Number of workers per job
exp_id: single_job
job_id: 1

hydra:
  run:
    dir: .  # Current directory or a specific directory where Ray Tune expects to find it
  sweep:
    dir: .  # Same as above
  output_subdir: null
  job_logging:
    level: DISABLE  # Disable job-specific logging
  hydra_logging:
    level: DISABLE  # Disable Hydra-specific logging

# hydra:  
#   output_subdir: null  
#   run:  
#     dir: .

# hydra:
#   mode: MULTIRUN
#   output_subdir: null
#   sweeper:
#     params:
#       training: resnet18
#       # training: resnet50, shufflenet, vgg16

# Disable Hydra logging while ensuring Ray Tune can still manage its directories
