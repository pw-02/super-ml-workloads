#dataloading configuration
name: lora_finetune_owt
s3_bucket: imagenet1k-sdl
s3_train_prefix: s3://owt-5mb-text-chunks/train
s3_val_prefix: s3://owt-5mb-text-chunks/val
num_pytorch_workers: 0
model_name: pythia-14m
run_training: True
run_validation: False
#trin
save_interval: 1000
log_interval: 1
global_batch_size: 16
micro_batch_size: 16
lr_warmup_steps: 50
epochs: 5
max_tokens: null
max_steps: 500
max_seq_length: 512
tie_embeddings: null
max_norm: null
min_lr:  4e-5
learning_rate: 4e-5
#eval
eval_interval: 1000
eval_max_new_tokens: 100
eval_max_iters: 100
eval_initial_validation: False
eval_final_validation: False

optimizer:  "AdamW"


checkpoint_dir: 'mlworkloads\checkpoints\EleutherAI\pythia-14m'  #'mlworkloads\language\checkpoints\EleutherAI\pythia-14m' #The path to the base model's checkpoint directory to load for finetuning.
out_dir: 
precision: 16-true #ossible choices: "bf16-true", "bf16-mixed", "32-true".
quantize: null
devices: 1 #How many devices/GPUs to use.
num_nodes: 1 #How many nodes the code is being run on.
lora_r: 8 
lora_alpha: 16
lora_dropout: 0.05
lora_query: True
lora_key: False
lora_value: True
lora_projection: False
lora_mlp: False
lora_head: False
