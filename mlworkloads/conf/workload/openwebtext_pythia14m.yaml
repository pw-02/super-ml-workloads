name: Pythia-14M/OpenWebText

#dataloading configuration
s3_bucket: owtchunks
train_dir: train/
val_dir: val/

#model configuration
model_architecture: pythia-14m
learning_rate: 4e-4
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
max_norm: 1.0
min_lr: 4e-5
max_seq_length: 512

#training configuration
max_tokens: 3e12  # 3 trillion
global_batch_size: 32 #Number of samples between optimizer steps across data-parallel ranks
micro_batch_size: 32 #Number of samples per data-parallel rank
lr_warmup_steps: 0
lr_warmup_fraction: null
tie_embeddings: False