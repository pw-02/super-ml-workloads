model_name: pythia-14m
log_interval: 1
global_batch_size: 125
micro_batch_size: 5
epochs: 1
epoch_size: 250
learning_rate: 6e-4
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
max_norm: 1.0
min_lr: 6e-5
eval_iterval: 1000
max_eval_iters: 50

# batch_size: 10
# max_epochs: 1
# max_minibatches_per_epoch: 10
# learning_rate: 6e-4
# weight_decay: 1e-1
# beta1: 0.9
# beta2: 0.95
# max_norm: 1.0
# min_lr: 6e-5
# shuffle: True
# num_workers: 0
# grad_acc_steps: null
